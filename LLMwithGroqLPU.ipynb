{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7ed4be",
   "metadata": {},
   "source": [
    "### Simple LLM App wiht LCEL\n",
    "\n",
    "In this quickstart we’ll show you how to build a simple LLM application with LangChain.\n",
    "This application will translate text from English into another language.\n",
    "This is a relatively simple LLM application – it’s just a single LLM call plus some prompting.\n",
    "Still, this is a great way to get started with LangChain – a lot of features can be built with just some prompting and an LLM call!\n",
    "\n",
    "After seeing this video, you’ll have a high-level overview of:\n",
    ". Using language models\n",
    "\n",
    ". Using PromptTemplates and OutputParsers\n",
    "\n",
    ". Using LangChain Expression Language (LCEL) to chain components together\n",
    "\n",
    ". Debugging and tracing your application using LangSmith\n",
    "\n",
    ". Deploying app on Langserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6e5587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## OpenAI API Key and Opensource models\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001F010F2E110>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001F010F2F580>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"gemma2-9b-it\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59915046",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Messages to LLM \n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English to French\"),\n",
    "    HumanMessage(content=\"Hello How are you?\")\n",
    "]\n",
    "\n",
    "response=model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are a few ways to say \"Hello, how are you?\" in French:\\n\\n**Formal:**\\n\\n* **Bonjour, comment allez-vous ?** (This is the most polite and formal way to greet someone.)\\n\\n**Informal:**\\n\\n* **Salut, ça va ?** (This is a common and casual greeting amongst friends.)\\n* **Coucou, comment vas-tu ?** (A more familiar and friendly greeting, often used with close friends.)\\n\\n\\n\\nLet me know if you\\'d like more variations!\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 109, 'prompt_tokens': 21, 'total_tokens': 130, 'completion_time': 0.198181818, 'prompt_time': 0.002142696, 'queue_time': 0.018349724, 'total_time': 0.200324514}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-652e379c-cb58-49a0-b8ec-b2b2783d88bb-0', usage_metadata={'input_tokens': 21, 'output_tokens': 109, 'total_tokens': 130})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are a few ways to say \"Hello, how are you?\" in French:\\n\\n**Formal:**\\n\\n* **Bonjour, comment allez-vous ?** (This is the most polite and formal way to greet someone.)\\n\\n**Informal:**\\n\\n* **Salut, ça va ?** (This is a common and casual greeting amongst friends.)\\n* **Coucou, comment vas-tu ?** (A more familiar and friendly greeting, often used with close friends.)\\n\\n\\n\\nLet me know if you\\'d like more variations!\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Parse the output to just get the response \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser=StrOutputParser()\n",
    "parser.invoke(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fea807c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are a few ways to say \"Hello, how are you?\" in French:\\n\\n**Formal:**\\n\\n* **Bonjour, comment allez-vous ?** (This is the most polite and formal way to greet someone.)\\n\\n**Informal:**\\n\\n* **Salut, comment vas-tu ?** (This is a more casual greeting, used with friends and family.)\\n* **Bonjour, ça va ?** (This is a very common and informal greeting.)\\n\\n**Other options:**\\n\\n* **Coucou, comment ça va ?** (This is a very informal and friendly greeting, often used with close friends.)\\n\\nThe best option will depend on the context and your relationship with the person you are greeting.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using LCEL to chain components\n",
    "chain=model|parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "380c9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chat Prompt Template\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "generic_template=\"Translate the following into {language}\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",generic_template),(\"user\",\"{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08098c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=prompt.invoke({\"language\":\"Roman Urdu\",\"text\":\"Hello,How are you\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into Roman Urdu', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hello,How are you', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s how you\\'d say \"Hello, How are you?\" in Roman Urdu:\\n\\n* **\"Salam, kaise hain?\"** \\n\\nLet me break it down:\\n\\n* **Salam:** This is the most common greeting in Urdu, meaning \"Hello\".\\n* **kaise hain:** This means \"How are you?\" and is a polite way to ask someone about their well-being.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Chaining using LCEL\n",
    "chain=prompt|model|parser\n",
    "\n",
    "chain.invoke({\"language\":\"Roman Urdu\",\"text\":\"Hello,How are you\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from fastapi) (2.11.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from fastapi) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: starlette, fastapi\n",
      "Successfully installed fastapi-0.115.12 starlette-0.46.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "524aaab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting click>=7.0 (from uvicorn)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from uvicorn) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Downloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Installing collected packages: click, uvicorn\n",
      "Successfully installed click-8.1.8 uvicorn-0.34.1\n"
     ]
    }
   ],
   "source": [
    "!pip install uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langserve\n",
      "  Using cached langserve-0.3.1-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.23.0 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langserve) (0.28.1)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langserve) (0.3.52)\n",
      "Requirement already satisfied: orjson<4,>=2 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langserve) (3.10.16)\n",
      "Requirement already satisfied: pydantic<3.0,>=2.7 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langserve) (2.11.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from httpx<1.0,>=0.23.0->langserve) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from httpx<1.0,>=0.23.0->langserve) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from httpx<1.0,>=0.23.0->langserve) (1.0.8)\n",
      "Requirement already satisfied: idna in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from httpx<1.0,>=0.23.0->langserve) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.23.0->langserve) (0.14.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langserve) (0.3.31)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langserve) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langserve) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langserve) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langserve) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langchain-core<0.4,>=0.3->langserve) (4.13.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from pydantic<3.0,>=2.7->langserve) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from pydantic<3.0,>=2.7->langserve) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from pydantic<3.0,>=2.7->langserve) (0.4.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3->langserve) (3.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3->langserve) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3->langserve) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3->langserve) (0.23.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from anyio->httpx<1.0,>=0.23.0->langserve) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from anyio->httpx<1.0,>=0.23.0->langserve) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3->langserve) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\faroo\\onedrive\\desktop\\lcel_app\\venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.3->langserve) (2.4.0)\n",
      "Using cached langserve-0.3.1-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: langserve\n",
      "Successfully installed langserve-0.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccfe9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
